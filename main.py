from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Optional, AsyncGenerator, Dict, Any
import httpx
import os
import json
from datetime import datetime
from pathlib import Path

app = FastAPI(title="Study Helper AI Service")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== MODELS ====================

class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    message: str
    subject: Optional[str] = "general"
    conversationHistory: Optional[List[ChatMessage]] = []
    stream: Optional[bool] = False

class TemplateRequest(BaseModel):
    template_id: str
    user_input: str
    subject: Optional[str] = "general"
    conversationHistory: Optional[List[ChatMessage]] = []
    stream: Optional[bool] = False

class ChatResponse(BaseModel):
    message: str
    conversationId: Optional[str] = None
    timestamp: str

# üÜï –ù–û–í–ê–Ø –ú–û–î–ï–õ–¨ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
class AnalyzeImageRequest(BaseModel):
    image_url: Optional[str] = None  # URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (Cloudinary, Imgur, etc)
    image_base64: Optional[str] = None  # –ò–ª–∏ base64 —Å—Ç—Ä–æ–∫–∞
    subject: Optional[str] = "general"  # –ü—Ä–µ–¥–º–µ—Ç –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

class AnalyzeImageResponse(BaseModel):
    description: str  # AI –æ–ø–∏—Å–∞–Ω–∏–µ —Å—Ç–∏–ª—è —Ä–µ—à–µ–Ω–∏—è
    timestamp: str
    success: bool

# ==================== CONFIGURATION ====================

OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY", "sk-or-v1-YOUR-KEY-HERE")
OPENROUTER_URL = "https://openrouter.ai/api/v1/chat/completions"

MODELS = {
    "math": "meta-llama/llama-3.3-70b-instruct:free",
    "programming": "qwen/qwen3-coder:free",
    "english": "deepseek/deepseek-chat-v3.1:free",
    "general": "meta-llama/llama-3.3-70b-instruct:free",
}

SUBJECT_PROMPTS = {
    "math": """You are an expert math tutor. Be patient, encouraging, and adapt explanations to the student's level.""",
    "programming": """You are an experienced programming mentor. Provide clear, practical code examples and explanations.""",
    "english": """You are an experienced English language tutor. Provide constructive feedback and practical examples.""",
    "general": """You are a helpful study assistant. Provide clear, accurate explanations and help students learn effectively."""
}


# ==================== TEMPLATE SERVICE ====================

class TemplateService:
    def __init__(self):
        self.templates_file = Path("templates.json")
        self.templates_cache = None
        self.load_templates()
    
    def load_templates(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —à–∞–±–ª–æ–Ω–æ–≤ –∏–∑ JSON —Ñ–∞–π–ª–∞"""
        try:
            if self.templates_file.exists():
                with open(self.templates_file, 'r', encoding='utf-8') as f:
                    self.templates_cache = json.load(f)
                print(f"‚úÖ Loaded {len(self.templates_cache.get('templates', []))} templates")
            else:
                print("‚ö†Ô∏è templates.json not found, using empty templates")
                self.templates_cache = {"templates": [], "categories": []}
        except Exception as e:
            print(f"‚ùå Error loading templates: {e}")
            self.templates_cache = {"templates": [], "categories": []}
    
    def get_all_templates(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ —à–∞–±–ª–æ–Ω—ã"""
        return self.templates_cache
    
    def get_template_by_id(self, template_id: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —à–∞–±–ª–æ–Ω –ø–æ ID"""
        templates = self.templates_cache.get("templates", [])
        for template in templates:
            if template["id"] == template_id:
                return template
        return None
    
    def get_templates_by_subject(self, subject: str) -> List[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å —à–∞–±–ª–æ–Ω—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø—Ä–µ–¥–º–µ—Ç–∞"""
        templates = self.templates_cache.get("templates", [])
        return [t for t in templates if subject in t.get("subjects", [])]
    
    def apply_template(self, template_id: str, user_input: str) -> str:
        """–ü—Ä–∏–º–µ–Ω–∏—Ç—å —à–∞–±–ª–æ–Ω —Å –ø–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–æ–π user_input"""
        template = self.get_template_by_id(template_id)
        if not template:
            raise ValueError(f"Template '{template_id}' not found")
        
        prompt = template["prompt"]
        
        # –ü–æ–¥—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤—Å–µ—Ö –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä–æ–≤
        for placeholder in template.get("placeholders", []):
            prompt = prompt.replace(f"{{{placeholder}}}", user_input)
        
        return prompt

# –°–æ–∑–¥–∞—ë–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
template_service = TemplateService()

# ==================== AI SERVICE ====================

async def call_openrouter(messages: List[dict], model: str) -> str:
    """–û–±—ã—á–Ω—ã–π –≤—ã–∑–æ–≤ OpenRouter"""
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
        "HTTP-Referer": "https://study-helper.app",
        "X-Title": "Study Helper"
    }
    
    payload = {
        "model": model,
        "messages": messages,
        "temperature": 0.7,
        "max_tokens": 3000
    }
    
    print(f"üöÄ [OpenRouter] Request to model: {model}")
    
    async with httpx.AsyncClient(timeout=60.0) as client:
        try:
            response = await client.post(OPENROUTER_URL, json=payload, headers=headers)
            response.raise_for_status()
            data = response.json()
            result = data["choices"][0]["message"]["content"]
            print(f"‚úÖ [OpenRouter] Success")
            return result
        except httpx.HTTPError as e:
            print(f"‚ùå [OpenRouter] Error: {str(e)}")
            raise HTTPException(status_code=500, detail=f"AI service error: {str(e)}")

def prepare_messages(message: str, subject: str, history: List[ChatMessage]) -> tuple[List[dict], str]:
    """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è AI"""
    model = MODELS.get(subject, MODELS["general"])
    system_prompt = SUBJECT_PROMPTS.get(subject, SUBJECT_PROMPTS["general"])
    
    messages = [{"role": "system", "content": system_prompt}]
    
    # –ò—Å—Ç–æ—Ä–∏—è
    if history:
        for msg in history[-10:]:
            messages.append({
                "role": msg.role,
                "content": msg.content
            })
    
    # –¢–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
    messages.append({
        "role": "user",
        "content": message
    })
    
    return messages, model

# ==================== ENDPOINTS ====================

@app.get("/")
async def root():
    """Root endpoint - –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ API"""
    return {
        "message": "Study Helper AI Service",
        "version": "1.1.0",
        "docs": "/docs"  # –°—Å—ã–ª–∫–∞ –Ω–∞ Swagger –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é
    }

@app.get("/health")
async def health():
    """Health check –¥–ª—è Docker –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
    return {
        "status": "ok",
        "service": "Study Helper AI Service",
        "version": "1.1.0",
        "features": ["chat", "templates", "streaming"]
    }

@app.get("/api/templates")
async def get_templates(subject: Optional[str] = None):
    """
    –ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ —à–∞–±–ª–æ–Ω—ã –∏–ª–∏ —à–∞–±–ª–æ–Ω—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø—Ä–µ–¥–º–µ—Ç–∞
    
    Query params:
    - subject (optional): math, programming, english, general
    """
    try:
        if subject:
            templates = template_service.get_templates_by_subject(subject)
            categories = template_service.templates_cache.get("categories", [])
            return {"templates": templates, "categories": categories}
        else:
            return template_service.get_all_templates()
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/templates/{template_id}")
async def get_template(template_id: str):
    """–ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —à–∞–±–ª–æ–Ω –ø–æ ID"""
    try:
        template = template_service.get_template_by_id(template_id)
        if not template:
            raise HTTPException(status_code=404, detail=f"Template '{template_id}' not found")
        return template
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/chat/template")
async def chat_with_template(request: TemplateRequest):
    """
    –û—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —à–∞–±–ª–æ–Ω–∞
    
    Body:
    {
      "template_id": "explain_topic",
      "user_input": "What is Newton's Third Law?",
      "subject": "physics",
      "conversationHistory": [],
      "stream": false
    }
    """
    try:
        print(f"üì• [Template Chat] template_id={request.template_id}, subject={request.subject}")
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —à–∞–±–ª–æ–Ω
        final_prompt = template_service.apply_template(request.template_id, request.user_input)
        print(f"üìù [Template] Applied prompt: {final_prompt[:100]}...")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è
        messages, model = prepare_messages(
            message=final_prompt,
            subject=request.subject,
            history=request.conversationHistory or []
        )
        
        # –í—ã–∑—ã–≤–∞–µ–º AI
        ai_response = await call_openrouter(messages, model)
        
        return ChatResponse(
            message=ai_response,
            timestamp=datetime.utcnow().isoformat()
        )
    
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        print(f"‚ùå [Template Chat] Error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/chat/send")
async def send_message(request: ChatRequest):
    """–û–±—ã—á–Ω—ã–π —á–∞—Ç –±–µ–∑ —à–∞–±–ª–æ–Ω–æ–≤"""
    try:
        print(f"üì• [Chat] subject={request.subject}")
        
        messages, model = prepare_messages(
            message=request.message,
            subject=request.subject,
            history=request.conversationHistory or []
        )
        
        ai_response = await call_openrouter(messages, model)
        
        return ChatResponse(
            message=ai_response,
            timestamp=datetime.utcnow().isoformat()
        )
    
    except Exception as e:
        print(f"‚ùå [Chat] Error: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/chat/stream")
async def chat_stream(request: ChatRequest):
    """
    Streaming chat endpoint - –æ—Ç–≤–µ—Ç –ø—Ä–∏—Ö–æ–¥–∏—Ç —á–∞—Å—Ç—è–º–∏ (Server-Sent Events)
    """
    async def generate():
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏–π –∏ –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
            messages, model = prepare_messages(
                request.message,
                request.subject,
                request.conversationHistory
            )
            
            print(f"üåä [Stream] Starting stream for model: {model}")
            
            # –ó–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è OpenRouter
            headers = {
                "Authorization": f"Bearer {OPENROUTER_API_KEY}",
                "Content-Type": "application/json",
                "HTTP-Referer": "https://study-helper.app",
                "X-Title": "Study Helper"
            }
            
            # Payload —Å –≤–∫–ª—é—á–µ–Ω–Ω—ã–º —Å—Ç—Ä–∏–º–∏–Ω–≥–æ–º
            payload = {
                "model": model,
                "messages": messages,
                "temperature": 0.7,
                "max_tokens": 3000,
                "stream": True
            }
            
            # –î–µ–ª–∞–µ–º streaming –∑–∞–ø—Ä–æ—Å –∫ OpenRouter
            async with httpx.AsyncClient(timeout=120.0) as client:
                async with client.stream(
                    "POST",
                    OPENROUTER_URL,
                    json=payload,
                    headers=headers
                ) as response:
                    response.raise_for_status()
                    
                    # –ß–∏—Ç–∞–µ–º –∏ –ø–µ—Ä–µ—Å—ã–ª–∞–µ–º –∫–∞–∂–¥—É—é —Å—Ç—Ä–æ–∫—É
                    async for line in response.aiter_lines():
                        if line.strip():
                            if line.startswith("data: "):
                                data_str = line[6:]
                                
                                if data_str == "[DONE]":
                                    print("‚úÖ [Stream] Completed")
                                    yield "data: [DONE]\n\n"
                                    break
                                
                                try:
                                    data_json = json.loads(data_str)
                                    if "choices" in data_json and len(data_json["choices"]) > 0:
                                        delta = data_json["choices"][0].get("delta", {})
                                        content = delta.get("content", "")
                                        if content:
                                            yield f"data: {json.dumps({'content': content})}\n\n"
                                except json.JSONDecodeError:
                                    continue
                                    
        except Exception as e:
            error_msg = f"Stream error: {str(e)}"
            print(f"‚ùå [Stream] {error_msg}")
            yield f"data: {json.dumps({'error': error_msg})}\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)